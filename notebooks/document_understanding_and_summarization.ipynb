{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "O9XnLNuDz-yR",
        "outputId": "26884299-cc5e-4edd-b952-3bec417caaab"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: langchain_core in /usr/local/lib/python3.12/dist-packages (1.2.6)\n",
            "Collecting langchain_community\n",
            "  Downloading langchain_community-0.4.1-py3-none-any.whl.metadata (3.0 kB)\n",
            "Collecting langchain-text-splitters\n",
            "  Downloading langchain_text_splitters-1.1.0-py3-none-any.whl.metadata (2.7 kB)\n",
            "Collecting langchain_google_genai\n",
            "  Downloading langchain_google_genai-4.2.0-py3-none-any.whl.metadata (2.7 kB)\n",
            "Collecting langchain_mongodb\n",
            "  Downloading langchain_mongodb-0.11.0-py3-none-any.whl.metadata (1.9 kB)\n",
            "Collecting docling\n",
            "  Downloading docling-2.68.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in /usr/local/lib/python3.12/dist-packages (from langchain_core) (1.33)\n",
            "Requirement already satisfied: langsmith<1.0.0,>=0.3.45 in /usr/local/lib/python3.12/dist-packages (from langchain_core) (0.6.1)\n",
            "Requirement already satisfied: packaging<26.0.0,>=23.2.0 in /usr/local/lib/python3.12/dist-packages (from langchain_core) (25.0)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.12/dist-packages (from langchain_core) (2.12.3)\n",
            "Requirement already satisfied: pyyaml<7.0.0,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from langchain_core) (6.0.3)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from langchain_core) (9.1.2)\n",
            "Requirement already satisfied: typing-extensions<5.0.0,>=4.7.0 in /usr/local/lib/python3.12/dist-packages (from langchain_core) (4.15.0)\n",
            "Requirement already satisfied: uuid-utils<1.0,>=0.12.0 in /usr/local/lib/python3.12/dist-packages (from langchain_core) (0.13.0)\n",
            "Collecting langchain-classic<2.0.0,>=1.0.0 (from langchain_community)\n",
            "  Downloading langchain_classic-1.0.1-py3-none-any.whl.metadata (4.2 kB)\n",
            "Requirement already satisfied: SQLAlchemy<3.0.0,>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from langchain_community) (2.0.45)\n",
            "Collecting requests<3.0.0,>=2.32.5 (from langchain_community)\n",
            "  Downloading requests-2.32.5-py3-none-any.whl.metadata (4.9 kB)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.12/dist-packages (from langchain_community) (3.13.3)\n",
            "Collecting dataclasses-json<0.7.0,>=0.6.7 (from langchain_community)\n",
            "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
            "Requirement already satisfied: pydantic-settings<3.0.0,>=2.10.1 in /usr/local/lib/python3.12/dist-packages (from langchain_community) (2.12.0)\n",
            "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from langchain_community) (0.4.3)\n",
            "Requirement already satisfied: numpy>=1.26.2 in /usr/local/lib/python3.12/dist-packages (from langchain_community) (2.0.2)\n",
            "Collecting filetype<2.0.0,>=1.2.0 (from langchain_google_genai)\n",
            "  Downloading filetype-1.2.0-py2.py3-none-any.whl.metadata (6.5 kB)\n",
            "Collecting google-genai<2.0.0,>=1.56.0 (from langchain_google_genai)\n",
            "  Downloading google_genai-1.59.0-py3-none-any.whl.metadata (53 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.1/53.1 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: langchain>=1.0 in /usr/local/lib/python3.12/dist-packages (from langchain_mongodb) (1.2.3)\n",
            "Requirement already satisfied: lark<2.0.0,>=1.1.9 in /usr/local/lib/python3.12/dist-packages (from langchain_mongodb) (1.3.1)\n",
            "Collecting pymongo-search-utils>=0.2.1 (from langchain_mongodb)\n",
            "  Downloading pymongo_search_utils-0.2.1-py3-none-any.whl.metadata (14 kB)\n",
            "Collecting pymongo>=4.6.1 (from langchain_mongodb)\n",
            "  Downloading pymongo-4.16.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (10.0 kB)\n",
            "Collecting docling-core<3.0.0,>=2.50.1 (from docling-core[chunking]<3.0.0,>=2.50.1->docling)\n",
            "  Downloading docling_core-2.59.0-py3-none-any.whl.metadata (7.7 kB)\n",
            "Collecting docling-parse<5.0.0,>=4.7.0 (from docling)\n",
            "  Downloading docling_parse-4.7.3-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (10 kB)\n",
            "Collecting docling-ibm-models<4,>=3.9.1 (from docling)\n",
            "  Downloading docling_ibm_models-3.10.3-py3-none-any.whl.metadata (7.3 kB)\n",
            "Collecting pypdfium2!=4.30.1,<5.0.0,>=4.30.0 (from docling)\n",
            "  Downloading pypdfium2-4.30.0-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (48 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.5/48.5 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: huggingface_hub<1,>=0.23 in /usr/local/lib/python3.12/dist-packages (from docling) (0.36.0)\n",
            "Collecting rapidocr<4.0.0,>=3.3 (from docling)\n",
            "  Downloading rapidocr-3.5.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: certifi>=2024.7.4 in /usr/local/lib/python3.12/dist-packages (from docling) (2026.1.4)\n",
            "Requirement already satisfied: rtree<2.0.0,>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from docling) (1.4.1)\n",
            "Collecting typer<0.20.0,>=0.12.5 (from docling)\n",
            "  Downloading typer-0.19.2-py3-none-any.whl.metadata (16 kB)\n",
            "Collecting python-docx<2.0.0,>=1.1.2 (from docling)\n",
            "  Downloading python_docx-1.2.0-py3-none-any.whl.metadata (2.0 kB)\n",
            "Collecting python-pptx<2.0.0,>=1.0.2 (from docling)\n",
            "  Downloading python_pptx-1.0.2-py3-none-any.whl.metadata (2.5 kB)\n",
            "Requirement already satisfied: beautifulsoup4<5.0.0,>=4.12.3 in /usr/local/lib/python3.12/dist-packages (from docling) (4.13.5)\n",
            "Requirement already satisfied: pandas<3.0.0,>=2.1.4 in /usr/local/lib/python3.12/dist-packages (from docling) (2.2.2)\n",
            "Collecting marko<3.0.0,>=2.1.2 (from docling)\n",
            "  Downloading marko-2.2.2-py3-none-any.whl.metadata (4.5 kB)\n",
            "Requirement already satisfied: openpyxl<4.0.0,>=3.1.5 in /usr/local/lib/python3.12/dist-packages (from docling) (3.1.5)\n",
            "Requirement already satisfied: lxml<7.0.0,>=4.0.0 in /usr/local/lib/python3.12/dist-packages (from docling) (6.0.2)\n",
            "Requirement already satisfied: pillow<12.0.0,>=10.0.0 in /usr/local/lib/python3.12/dist-packages (from docling) (11.3.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.65.0 in /usr/local/lib/python3.12/dist-packages (from docling) (4.67.1)\n",
            "Requirement already satisfied: pluggy<2.0.0,>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from docling) (1.6.0)\n",
            "Collecting pylatexenc<3.0,>=2.10 (from docling)\n",
            "  Downloading pylatexenc-2.10.tar.gz (162 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m162.6/162.6 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: scipy<2.0.0,>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from docling) (1.16.3)\n",
            "Requirement already satisfied: accelerate<2,>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from docling) (1.12.0)\n",
            "Collecting polyfactory>=2.22.2 (from docling)\n",
            "  Downloading polyfactory-3.2.0-py3-none-any.whl.metadata (27 kB)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from accelerate<2,>=1.0.0->docling) (5.9.5)\n",
            "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from accelerate<2,>=1.0.0->docling) (2.9.0+cpu)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from accelerate<2,>=1.0.0->docling) (0.7.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.22.0)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4<5.0.0,>=4.12.3->docling) (2.8.1)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7.0,>=0.6.7->langchain_community)\n",
            "  Downloading marshmallow-3.26.2-py3-none-any.whl.metadata (7.3 kB)\n",
            "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7.0,>=0.6.7->langchain_community)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: jsonschema<5.0.0,>=4.16.0 in /usr/local/lib/python3.12/dist-packages (from docling-core<3.0.0,>=2.50.1->docling-core[chunking]<3.0.0,>=2.50.1->docling) (4.26.0)\n",
            "Collecting jsonref<2.0.0,>=1.1.0 (from docling-core<3.0.0,>=2.50.1->docling-core[chunking]<3.0.0,>=2.50.1->docling)\n",
            "  Downloading jsonref-1.1.0-py3-none-any.whl.metadata (2.7 kB)\n",
            "Requirement already satisfied: tabulate<0.10.0,>=0.9.0 in /usr/local/lib/python3.12/dist-packages (from docling-core<3.0.0,>=2.50.1->docling-core[chunking]<3.0.0,>=2.50.1->docling) (0.9.0)\n",
            "Collecting latex2mathml<4.0.0,>=3.77.0 (from docling-core<3.0.0,>=2.50.1->docling-core[chunking]<3.0.0,>=2.50.1->docling)\n",
            "  Downloading latex2mathml-3.78.1-py3-none-any.whl.metadata (15 kB)\n",
            "Collecting semchunk<3.0.0,>=2.2.0 (from docling-core[chunking]<3.0.0,>=2.50.1->docling)\n",
            "  Downloading semchunk-2.2.2-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting tree-sitter<1.0.0,>=0.23.2 (from docling-core[chunking]<3.0.0,>=2.50.1->docling)\n",
            "  Downloading tree_sitter-0.25.2-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (10.0 kB)\n",
            "Collecting tree-sitter-python<1.0.0,>=0.23.6 (from docling-core[chunking]<3.0.0,>=2.50.1->docling)\n",
            "  Downloading tree_sitter_python-0.25.0-cp310-abi3-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl.metadata (1.9 kB)\n",
            "Collecting tree-sitter-c<1.0.0,>=0.23.4 (from docling-core[chunking]<3.0.0,>=2.50.1->docling)\n",
            "  Downloading tree_sitter_c-0.24.1-cp310-abi3-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting tree-sitter-javascript<1.0.0,>=0.23.1 (from docling-core[chunking]<3.0.0,>=2.50.1->docling)\n",
            "  Downloading tree_sitter_javascript-0.25.0-cp310-abi3-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl.metadata (2.2 kB)\n",
            "Collecting tree-sitter-typescript<1.0.0,>=0.23.2 (from docling-core[chunking]<3.0.0,>=2.50.1->docling)\n",
            "  Downloading tree_sitter_typescript-0.23.2-cp39-abi3-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.3 kB)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.34.0 in /usr/local/lib/python3.12/dist-packages (from docling-core[chunking]<3.0.0,>=2.50.1->docling) (4.57.3)\n",
            "Requirement already satisfied: torchvision<1,>=0 in /usr/local/lib/python3.12/dist-packages (from docling-ibm-models<4,>=3.9.1->docling) (0.24.0+cpu)\n",
            "Collecting jsonlines<5.0.0,>=3.1.0 (from docling-ibm-models<4,>=3.9.1->docling)\n",
            "  Downloading jsonlines-4.0.0-py3-none-any.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: anyio<5.0.0,>=4.8.0 in /usr/local/lib/python3.12/dist-packages (from google-genai<2.0.0,>=1.56.0->langchain_google_genai) (4.12.1)\n",
            "Collecting google-auth<3.0.0,>=2.47.0 (from google-auth[requests]<3.0.0,>=2.47.0->google-genai<2.0.0,>=1.56.0->langchain_google_genai)\n",
            "  Downloading google_auth-2.47.0-py3-none-any.whl.metadata (6.4 kB)\n",
            "Requirement already satisfied: httpx<1.0.0,>=0.28.1 in /usr/local/lib/python3.12/dist-packages (from google-genai<2.0.0,>=1.56.0->langchain_google_genai) (0.28.1)\n",
            "Requirement already satisfied: websockets<15.1.0,>=13.0.0 in /usr/local/lib/python3.12/dist-packages (from google-genai<2.0.0,>=1.56.0->langchain_google_genai) (15.0.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from google-genai<2.0.0,>=1.56.0->langchain_google_genai) (1.9.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from google-genai<2.0.0,>=1.56.0->langchain_google_genai) (1.3.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface_hub<1,>=0.23->docling) (3.20.2)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub<1,>=0.23->docling) (2025.3.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub<1,>=0.23->docling) (1.2.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch<2.0.0,>=1.33.0->langchain_core) (3.0.0)\n",
            "Requirement already satisfied: langgraph<1.1.0,>=1.0.2 in /usr/local/lib/python3.12/dist-packages (from langchain>=1.0->langchain_mongodb) (1.0.5)\n",
            "Requirement already satisfied: orjson>=3.9.14 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain_core) (3.11.5)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain_core) (1.0.0)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain_core) (0.25.0)\n",
            "Requirement already satisfied: et-xmlfile in /usr/local/lib/python3.12/dist-packages (from openpyxl<4.0.0,>=3.1.5->docling) (2.0.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas<3.0.0,>=2.1.4->docling) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas<3.0.0,>=2.1.4->docling) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas<3.0.0,>=2.1.4->docling) (2025.3)\n",
            "Collecting faker>=5.0.0 (from polyfactory>=2.22.2->docling)\n",
            "  Downloading faker-40.1.2-py3-none-any.whl.metadata (16 kB)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain_core) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain_core) (2.41.4)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain_core) (0.4.2)\n",
            "Requirement already satisfied: python-dotenv>=0.21.0 in /usr/local/lib/python3.12/dist-packages (from pydantic-settings<3.0.0,>=2.10.1->langchain_community) (1.2.1)\n",
            "Collecting dnspython<3.0.0,>=2.6.1 (from pymongo>=4.6.1->langchain_mongodb)\n",
            "  Downloading dnspython-2.8.0-py3-none-any.whl.metadata (5.7 kB)\n",
            "Collecting XlsxWriter>=0.5.7 (from python-pptx<2.0.0,>=1.0.2->docling)\n",
            "  Downloading xlsxwriter-3.2.9-py3-none-any.whl.metadata (2.7 kB)\n",
            "Collecting pyclipper>=1.2.0 (from rapidocr<4.0.0,>=3.3->docling)\n",
            "  Downloading pyclipper-1.4.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (8.6 kB)\n",
            "Requirement already satisfied: opencv_python>=4.5.1.48 in /usr/local/lib/python3.12/dist-packages (from rapidocr<4.0.0,>=3.3->docling) (4.12.0.88)\n",
            "Requirement already satisfied: six>=1.15.0 in /usr/local/lib/python3.12/dist-packages (from rapidocr<4.0.0,>=3.3->docling) (1.17.0)\n",
            "Requirement already satisfied: Shapely!=2.0.4,>=1.7.1 in /usr/local/lib/python3.12/dist-packages (from rapidocr<4.0.0,>=3.3->docling) (2.1.2)\n",
            "Requirement already satisfied: omegaconf in /usr/local/lib/python3.12/dist-packages (from rapidocr<4.0.0,>=3.3->docling) (2.3.0)\n",
            "Collecting colorlog (from rapidocr<4.0.0,>=3.3->docling)\n",
            "  Downloading colorlog-6.10.1-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.32.5->langchain_community) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.32.5->langchain_community) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.32.5->langchain_community) (2.5.0)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.12/dist-packages (from SQLAlchemy<3.0.0,>=1.4.0->langchain_community) (3.3.0)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.12/dist-packages (from typer<0.20.0,>=0.12.5->docling) (8.3.1)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from typer<0.20.0,>=0.12.5->docling) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.12/dist-packages (from typer<0.20.0,>=0.12.5->docling) (13.9.4)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from google-auth<3.0.0,>=2.47.0->google-auth[requests]<3.0.0,>=2.47.0->google-genai<2.0.0,>=1.56.0->langchain_google_genai) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.12/dist-packages (from google-auth<3.0.0,>=2.47.0->google-auth[requests]<3.0.0,>=2.47.0->google-genai<2.0.0,>=1.56.0->langchain_google_genai) (4.9.1)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0,>=0.28.1->google-genai<2.0.0,>=1.56.0->langchain_google_genai) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1.0.0,>=0.28.1->google-genai<2.0.0,>=1.56.0->langchain_google_genai) (0.16.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.12/dist-packages (from jsonschema<5.0.0,>=4.16.0->docling-core<3.0.0,>=2.50.1->docling-core[chunking]<3.0.0,>=2.50.1->docling) (2025.9.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.12/dist-packages (from jsonschema<5.0.0,>=4.16.0->docling-core<3.0.0,>=2.50.1->docling-core[chunking]<3.0.0,>=2.50.1->docling) (0.37.0)\n",
            "Requirement already satisfied: rpds-py>=0.25.0 in /usr/local/lib/python3.12/dist-packages (from jsonschema<5.0.0,>=4.16.0->docling-core<3.0.0,>=2.50.1->docling-core[chunking]<3.0.0,>=2.50.1->docling) (0.30.0)\n",
            "Requirement already satisfied: langgraph-checkpoint<4.0.0,>=2.1.0 in /usr/local/lib/python3.12/dist-packages (from langgraph<1.1.0,>=1.0.2->langchain>=1.0->langchain_mongodb) (3.0.1)\n",
            "Requirement already satisfied: langgraph-prebuilt<1.1.0,>=1.0.2 in /usr/local/lib/python3.12/dist-packages (from langgraph<1.1.0,>=1.0.2->langchain>=1.0->langchain_mongodb) (1.0.5)\n",
            "Requirement already satisfied: langgraph-sdk<0.4.0,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from langgraph<1.1.0,>=1.0.2->langchain>=1.0->langchain_mongodb) (0.3.1)\n",
            "Requirement already satisfied: xxhash>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from langgraph<1.1.0,>=1.0.2->langchain>=1.0->langchain_mongodb) (3.6.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->typer<0.20.0,>=0.12.5->docling) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->typer<0.20.0,>=0.12.5->docling) (2.19.2)\n",
            "Collecting mpire[dill] (from semchunk<3.0.0,>=2.2.0->docling-core[chunking]<3.0.0,>=2.50.1->docling)\n",
            "  Downloading mpire-2.10.2-py3-none-any.whl.metadata (14 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate<2,>=1.0.0->docling) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate<2,>=1.0.0->docling) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate<2,>=1.0.0->docling) (3.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate<2,>=1.0.0->docling) (3.1.6)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers<5.0.0,>=4.34.0->docling-core[chunking]<3.0.0,>=2.50.1->docling) (2025.11.3)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers<5.0.0,>=4.34.0->docling-core[chunking]<3.0.0,>=2.50.1->docling) (0.22.2)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7.0,>=0.6.7->langchain_community)\n",
            "  Downloading mypy_extensions-1.1.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Requirement already satisfied: antlr4-python3-runtime==4.9.* in /usr/local/lib/python3.12/dist-packages (from omegaconf->rapidocr<4.0.0,>=3.3->docling) (4.9.3)\n",
            "Requirement already satisfied: ormsgpack>=1.12.0 in /usr/local/lib/python3.12/dist-packages (from langgraph-checkpoint<4.0.0,>=2.1.0->langgraph<1.1.0,>=1.0.2->langchain>=1.0->langchain_mongodb) (1.12.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<0.20.0,>=0.12.5->docling) (0.1.2)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.12/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3.0.0,>=2.47.0->google-auth[requests]<3.0.0,>=2.47.0->google-genai<2.0.0,>=1.56.0->langchain_google_genai) (0.6.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=2.0.0->accelerate<2,>=1.0.0->docling) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=2.0.0->accelerate<2,>=1.0.0->docling) (3.0.3)\n",
            "Requirement already satisfied: multiprocess>=0.70.15 in /usr/local/lib/python3.12/dist-packages (from mpire[dill]->semchunk<3.0.0,>=2.2.0->docling-core[chunking]<3.0.0,>=2.50.1->docling) (0.70.16)\n",
            "Requirement already satisfied: dill>=0.3.8 in /usr/local/lib/python3.12/dist-packages (from multiprocess>=0.70.15->mpire[dill]->semchunk<3.0.0,>=2.2.0->docling-core[chunking]<3.0.0,>=2.50.1->docling) (0.3.8)\n",
            "Downloading langchain_community-0.4.1-py3-none-any.whl (2.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m34.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_text_splitters-1.1.0-py3-none-any.whl (34 kB)\n",
            "Downloading langchain_google_genai-4.2.0-py3-none-any.whl (66 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.5/66.5 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_mongodb-0.11.0-py3-none-any.whl (62 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.0/62.0 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading docling-2.68.0-py3-none-any.whl (282 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m282.9/282.9 kB\u001b[0m \u001b[31m17.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
            "Downloading docling_core-2.59.0-py3-none-any.whl (223 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m223.3/223.3 kB\u001b[0m \u001b[31m14.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading docling_ibm_models-3.10.3-py3-none-any.whl (87 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m87.4/87.4 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading docling_parse-4.7.3-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (15.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.1/15.1 MB\u001b[0m \u001b[31m73.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading filetype-1.2.0-py2.py3-none-any.whl (19 kB)\n",
            "Downloading google_genai-1.59.0-py3-none-any.whl (719 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m719.1/719.1 kB\u001b[0m \u001b[31m42.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_classic-1.0.1-py3-none-any.whl (1.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m39.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading marko-2.2.2-py3-none-any.whl (42 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.7/42.7 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading polyfactory-3.2.0-py3-none-any.whl (62 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.2/62.2 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pymongo-4.16.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (1.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m63.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pymongo_search_utils-0.2.1-py3-none-any.whl (17 kB)\n",
            "Downloading pypdfium2-4.30.0-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.8/2.8 MB\u001b[0m \u001b[31m77.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_docx-1.2.0-py3-none-any.whl (252 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m253.0/253.0 kB\u001b[0m \u001b[31m20.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_pptx-1.0.2-py3-none-any.whl (472 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m472.8/472.8 kB\u001b[0m \u001b[31m35.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading rapidocr-3.5.0-py3-none-any.whl (15.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.1/15.1 MB\u001b[0m \u001b[31m97.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading requests-2.32.5-py3-none-any.whl (64 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.7/64.7 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading typer-0.19.2-py3-none-any.whl (46 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.7/46.7 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dnspython-2.8.0-py3-none-any.whl (331 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m331.1/331.1 kB\u001b[0m \u001b[31m22.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading faker-40.1.2-py3-none-any.whl (2.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m74.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading google_auth-2.47.0-py3-none-any.whl (234 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m234.9/234.9 kB\u001b[0m \u001b[31m20.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jsonlines-4.0.0-py3-none-any.whl (8.7 kB)\n",
            "Downloading jsonref-1.1.0-py3-none-any.whl (9.4 kB)\n",
            "Downloading latex2mathml-3.78.1-py3-none-any.whl (73 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.9/73.9 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading marshmallow-3.26.2-py3-none-any.whl (50 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.0/51.0 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyclipper-1.4.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (978 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m978.2/978.2 kB\u001b[0m \u001b[31m59.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading semchunk-2.2.2-py3-none-any.whl (10 kB)\n",
            "Downloading tree_sitter-0.25.2-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (635 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m635.4/635.4 kB\u001b[0m \u001b[31m45.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tree_sitter_c-0.24.1-cp310-abi3-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (98 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.1/98.1 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tree_sitter_javascript-0.25.0-cp310-abi3-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl (99 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.7/99.7 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tree_sitter_python-0.25.0-cp310-abi3-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl (108 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m108.1/108.1 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tree_sitter_typescript-0.23.2-cp39-abi3-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (344 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m345.0/345.0 kB\u001b[0m \u001b[31m21.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Downloading xlsxwriter-3.2.9-py3-none-any.whl (175 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m175.3/175.3 kB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading colorlog-6.10.1-py3-none-any.whl (11 kB)\n",
            "Downloading mypy_extensions-1.1.0-py3-none-any.whl (5.0 kB)\n",
            "Downloading mpire-2.10.2-py3-none-any.whl (272 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m272.8/272.8 kB\u001b[0m \u001b[31m20.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: pylatexenc\n",
            "  Building wheel for pylatexenc (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pylatexenc: filename=pylatexenc-2.10-py3-none-any.whl size=136817 sha256=901ea27c9283118c5452d640a2f5a3591ea11480b436a0fb31cd21f2ff4cd2f5\n",
            "  Stored in directory: /root/.cache/pip/wheels/06/3e/78/fa1588c1ae991bbfd814af2bcac6cef7a178beee1939180d46\n",
            "Successfully built pylatexenc\n",
            "Installing collected packages: pylatexenc, filetype, XlsxWriter, tree-sitter-typescript, tree-sitter-python, tree-sitter-javascript, tree-sitter-c, tree-sitter, requests, python-docx, pypdfium2, pyclipper, mypy-extensions, mpire, marshmallow, marko, latex2mathml, jsonref, jsonlines, faker, dnspython, colorlog, typing-inspect, rapidocr, python-pptx, pymongo, polyfactory, google-auth, typer, semchunk, pymongo-search-utils, dataclasses-json, google-genai, docling-core, langchain-text-splitters, langchain_google_genai, docling-parse, docling-ibm-models, langchain-classic, docling, langchain_community, langchain_mongodb\n",
            "  Attempting uninstall: requests\n",
            "    Found existing installation: requests 2.32.4\n",
            "    Uninstalling requests-2.32.4:\n",
            "      Successfully uninstalled requests-2.32.4\n",
            "  Attempting uninstall: google-auth\n",
            "    Found existing installation: google-auth 2.43.0\n",
            "    Uninstalling google-auth-2.43.0:\n",
            "      Successfully uninstalled google-auth-2.43.0\n",
            "  Attempting uninstall: typer\n",
            "    Found existing installation: typer 0.21.1\n",
            "    Uninstalling typer-0.21.1:\n",
            "      Successfully uninstalled typer-0.21.1\n",
            "  Attempting uninstall: google-genai\n",
            "    Found existing installation: google-genai 1.55.0\n",
            "    Uninstalling google-genai-1.55.0:\n",
            "      Successfully uninstalled google-genai-1.55.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires google-auth==2.43.0, but you have google-auth 2.47.0 which is incompatible.\n",
            "google-colab 1.0.0 requires requests==2.32.4, but you have requests 2.32.5 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed XlsxWriter-3.2.9 colorlog-6.10.1 dataclasses-json-0.6.7 dnspython-2.8.0 docling-2.68.0 docling-core-2.59.0 docling-ibm-models-3.10.3 docling-parse-4.7.3 faker-40.1.2 filetype-1.2.0 google-auth-2.47.0 google-genai-1.59.0 jsonlines-4.0.0 jsonref-1.1.0 langchain-classic-1.0.1 langchain-text-splitters-1.1.0 langchain_community-0.4.1 langchain_google_genai-4.2.0 langchain_mongodb-0.11.0 latex2mathml-3.78.1 marko-2.2.2 marshmallow-3.26.2 mpire-2.10.2 mypy-extensions-1.1.0 polyfactory-3.2.0 pyclipper-1.4.0 pylatexenc-2.10 pymongo-4.16.0 pymongo-search-utils-0.2.1 pypdfium2-4.30.0 python-docx-1.2.0 python-pptx-1.0.2 rapidocr-3.5.0 requests-2.32.5 semchunk-2.2.2 tree-sitter-0.25.2 tree-sitter-c-0.24.1 tree-sitter-javascript-0.25.0 tree-sitter-python-0.25.0 tree-sitter-typescript-0.23.2 typer-0.19.2 typing-inspect-0.9.0\n"
          ]
        },
        {
          "data": {
            "application/vnd.colab-display-data+json": {
              "id": "11400a2343734039bc090867deb568fe",
              "pip_warning": {
                "packages": [
                  "google"
                ]
              }
            }
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "!pip install langchain_core langchain_community langchain-text-splitters langchain_google_genai langchain_mongodb docling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kbx_0gfqW7US"
      },
      "source": [
        "# Stage 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "spQX-nPw1gCT",
        "outputId": "90f15be1-ff61-4ba8-a9c3-e31f5f027d92"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[INFO] 2026-01-17 17:43:52,213 [RapidOCR] base.py:22: Using engine_name: torch\u001b[0m\n",
            "\u001b[32m[INFO] 2026-01-17 17:43:52,218 [RapidOCR] device_config.py:50: Using CPU device\u001b[0m\n",
            "\u001b[32m[INFO] 2026-01-17 17:43:52,223 [RapidOCR] download_file.py:68: Initiating download: https://www.modelscope.cn/models/RapidAI/RapidOCR/resolve/v3.5.0/torch/PP-OCRv4/det/ch_PP-OCRv4_det_infer.pth\u001b[0m\n",
            "\u001b[32m[INFO] 2026-01-17 17:43:53,974 [RapidOCR] download_file.py:82: Download size: 13.83MB\u001b[0m\n",
            "\u001b[32m[INFO] 2026-01-17 17:43:54,850 [RapidOCR] download_file.py:95: Successfully saved to: /usr/local/lib/python3.12/dist-packages/rapidocr/models/ch_PP-OCRv4_det_infer.pth\u001b[0m\n",
            "\u001b[32m[INFO] 2026-01-17 17:43:54,855 [RapidOCR] main.py:50: Using /usr/local/lib/python3.12/dist-packages/rapidocr/models/ch_PP-OCRv4_det_infer.pth\u001b[0m\n",
            "\u001b[32m[INFO] 2026-01-17 17:43:55,213 [RapidOCR] base.py:22: Using engine_name: torch\u001b[0m\n",
            "\u001b[32m[INFO] 2026-01-17 17:43:55,214 [RapidOCR] device_config.py:50: Using CPU device\u001b[0m\n",
            "\u001b[32m[INFO] 2026-01-17 17:43:55,215 [RapidOCR] download_file.py:68: Initiating download: https://www.modelscope.cn/models/RapidAI/RapidOCR/resolve/v3.5.0/torch/PP-OCRv4/cls/ch_ptocr_mobile_v2.0_cls_infer.pth\u001b[0m\n",
            "\u001b[32m[INFO] 2026-01-17 17:43:56,397 [RapidOCR] download_file.py:82: Download size: 0.56MB\u001b[0m\n",
            "\u001b[32m[INFO] 2026-01-17 17:43:56,475 [RapidOCR] download_file.py:95: Successfully saved to: /usr/local/lib/python3.12/dist-packages/rapidocr/models/ch_ptocr_mobile_v2.0_cls_infer.pth\u001b[0m\n",
            "\u001b[32m[INFO] 2026-01-17 17:43:56,481 [RapidOCR] main.py:50: Using /usr/local/lib/python3.12/dist-packages/rapidocr/models/ch_ptocr_mobile_v2.0_cls_infer.pth\u001b[0m\n",
            "\u001b[32m[INFO] 2026-01-17 17:43:56,630 [RapidOCR] base.py:22: Using engine_name: torch\u001b[0m\n",
            "\u001b[32m[INFO] 2026-01-17 17:43:56,631 [RapidOCR] device_config.py:50: Using CPU device\u001b[0m\n",
            "\u001b[32m[INFO] 2026-01-17 17:43:56,634 [RapidOCR] download_file.py:68: Initiating download: https://www.modelscope.cn/models/RapidAI/RapidOCR/resolve/v3.5.0/torch/PP-OCRv4/rec/ch_PP-OCRv4_rec_infer.pth\u001b[0m\n",
            "\u001b[32m[INFO] 2026-01-17 17:43:57,842 [RapidOCR] download_file.py:82: Download size: 25.67MB\u001b[0m\n",
            "\u001b[32m[INFO] 2026-01-17 17:43:59,366 [RapidOCR] download_file.py:95: Successfully saved to: /usr/local/lib/python3.12/dist-packages/rapidocr/models/ch_PP-OCRv4_rec_infer.pth\u001b[0m\n",
            "\u001b[32m[INFO] 2026-01-17 17:43:59,370 [RapidOCR] main.py:50: Using /usr/local/lib/python3.12/dist-packages/rapidocr/models/ch_PP-OCRv4_rec_infer.pth\u001b[0m\n",
            "WARNING:torchao.kernel.intmm:Warning: Detected no triton, on systems without Triton certain kernels will not work\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<!-- image -->\n",
            "\n",
            "## Intelligent Document Understanding &amp; Summarization Platform\n",
            "\n",
            "## 1. Introduction\n",
            "\n",
            "Organizations today generate and consume massive volumes of unstructured textual data in the form of reports, policies, research papers, legal contracts, compliance documents, and strategic briefs. While digital storage has made access easier, extracting meaningful insights from these large  and  complex  documents  remains  a  significant  challenge.  Professionals  often  spend considerable \n"
          ]
        }
      ],
      "source": [
        "from docling.document_converter import DocumentConverter\n",
        "\n",
        "# Stage 1: Local Ingestion & Parsing\n",
        "source = \"Team Alpha.pdf\"  # Local file path\n",
        "converter = DocumentConverter()\n",
        "result = converter.convert(source)\n",
        "\n",
        "# Export to Markdown (perfect for maintaining section headers)\n",
        "markdown_output = result.document.export_to_markdown()\n",
        "print(markdown_output[:500])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LZmSmY7z3Ok1",
        "outputId": "8cfbf793-dcc2-496f-aae9-dbfabd2379c5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<!-- image -->\n",
            "\n",
            "## Intelligent Document Understanding &amp; Summarization Platform\n",
            "\n",
            "## 1. Introduction\n",
            "\n",
            "Organizations today generate and consume massive volumes of unstructured textual data in the form of reports, policies, research papers, legal contracts, compliance documents, and strategic briefs. While digital storage has made access easier, extracting meaningful insights from these large  and  complex  documents  remains  a  significant  challenge.  Professionals  often  spend considerable time reading, summarizing, and interpreting documents to identify key decisions, risks, deadlines, and recommendations.\n",
            "\n",
            "The Intelligent  Document Understanding &amp; Summarization Platform aims  to  address  this challenge by providing an end-to-end Natural Language Processing (NLP) solution capable of understanding,  analyzing,  and  summarizing  long-form  documents.  The  platform  goes  beyond basic  text  summarization  by  preserving  semantic  context  across  sections,  extracting  structured insights, and enabling intelligent search through a document repository. Designed as a web-based enterprise solution, the system supports researchers, legal teams, policy analysts, and corporate decision-makers in transforming unstructured documents into actionable knowledge.\n",
            "\n",
            "## 2. Problem Statement\n",
            "\n",
            "Despite advancements in NLP, most existing document analysis tools face significant limitations when applied to real-world, long-form documents:\n",
            "\n",
            "- Large documents exceed the context window of standard language models, leading to loss of meaning.\n",
            "- Simple  summarization  tools  produce  generic  outputs  that  miss  critical  details  and relationships.\n",
            "- Important actionable insights such as deadlines, obligations, risks, and decisions are not explicitly extracted.\n",
            "- Documents  are  stored  as  static  files,  making  it  difficult  to  query  historical  content intelligently.\n",
            "- Lack of structure in outputs prevents seamless integration with enterprise workflows and analytics systems.\n",
            "\n",
            "These  challenges  result  in  inefficiencies,  missed  insights,  and  increased  cognitive  load  for professionals who rely on accurate and timely document understanding. There is a strong need for an  intelligent  platform  that  can  deeply  understand  documents,  preserve  long-range  context, generate structured summaries, and support natural language interaction with stored content.\n",
            "\n",
            "## 3. Methodology\n",
            "\n",
            "The proposed platform will be developed using a modular, scalable NLP architecture designed to handle long documents, maintain semantic consistency, and produce structured outputs.\n",
            "\n",
            "## a. Document Ingestion and Segmentation\n",
            "\n",
            "- Support uploading of PDF and DOC files (single or multiple).\n",
            "- Extract raw text using document parsers while preserving layout and section boundaries.\n",
            "- Automatically  segment  documents  into  logical  sections  (e.g.,  abstract,  introduction, clauses, conclusions) using layout cues and semantic segmentation models.\n",
            "- Apply chunking  strategies  with  overlap to  manage  long-context  documents  while maintaining continuity.\n",
            "\n",
            "## b. Semantic Understanding and Information Extraction\n",
            "\n",
            "- Use transformer-based NLP models (e.g., Longformer, BigBird, or LLM-based pipelines) to capture document-level semantics.\n",
            "- Perform topic modeling to identify major themes and subject areas.\n",
            "- Apply Named Entity Recognition (NER) to extract entities such as organizations, dates, monetary values, legal references, and key stakeholders.\n",
            "- Detect  relationships  between  entities  to  understand  dependencies,  obligations,  and responsibilities.\n",
            "\n",
            "## c. Summarization and Insight Generation\n",
            "\n",
            "- Generate multiple types of summaries:\n",
            "- o Short executive summary for quick understanding\n",
            "- o Detailed summary preserving technical depth\n",
            "- o Section-wise summaries aligned with document structure\n",
            "- Extract actionable insights , including:\n",
            "- o Key decisions and conclusions\n",
            "- o Deadlines and timelines\n",
            "- o Identified risks and constraints\n",
            "- o Recommendations and next steps\n",
            "- Output results in structured formats (JSON) alongside human-readable summaries.\n",
            "\n",
            "<!-- image -->\n",
            "\n",
            "## d. Searchable Repository and Web Platform\n",
            "\n",
            "- Store processed documents and extracted metadata in a searchable repository.\n",
            "- Enable natural language querying across previously uploaded documents.\n",
            "- Develop a web-based interface using Flask or FastAPI for document upload, processing visualization, summary viewing, and querying.\n",
            "- Implement access control, document versioning, and audit logging for enterprise readiness.\n",
            "\n",
            "## 4. Outcomes\n",
            "\n",
            "The successful implementation of this project will deliver:\n",
            "\n",
            "- An end-to-end  intelligent  document  understanding  platform for  long  and  complex documents\n",
            "- Accurate, context-preserving multi-level summarization\n",
            "- Structured extraction of entities, relationships, and actionable insights\n",
            "- A searchable, queryable document repository powered by NLP\n",
            "- A scalable, web-based solution suitable for enterprise, research, and legal use cases\n",
            "\n",
            "This  platform  demonstrates  advanced  expertise  in  long-context  NLP,  document  intelligence, semantic analysis, and applied AI system design. It serves as a strong foundation for enterprise document automation, decision-support systems, and AI-driven knowledge management solutions.\n",
            "\n",
            "<!-- image -->\n",
            "\n",
            "<!-- image -->\n"
          ]
        }
      ],
      "source": [
        "print(markdown_output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1zPD1-2X4v0e",
        "outputId": "f7c199cf-7507-425d-8392-0d8b350ff7c1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Section Found: {}\n",
            "Content Preview: <!-- image -->...\n",
            "\n",
            "Section Found: {'Header 2': 'Intelligent Document Understanding &amp; Summarization Platform'}\n",
            "Content Preview: ## Intelligent Document Understanding &amp; Summarization Platform...\n",
            "\n",
            "Section Found: {'Header 2': '1. Introduction'}\n",
            "Content Preview: ## 1. Introduction  \n",
            "Organizations today generate and consume massive volumes of unstructured textua...\n",
            "\n",
            "Section Found: {'Header 2': '2. Problem Statement'}\n",
            "Content Preview: ## 2. Problem Statement  \n",
            "Despite advancements in NLP, most existing document analysis tools face si...\n",
            "\n",
            "Section Found: {'Header 2': '3. Methodology'}\n",
            "Content Preview: ## 3. Methodology  \n",
            "The proposed platform will be developed using a modular, scalable NLP architectu...\n",
            "\n",
            "Section Found: {'Header 2': 'a. Document Ingestion and Segmentation'}\n",
            "Content Preview: ## a. Document Ingestion and Segmentation  \n",
            "- Support uploading of PDF and DOC files (single or mult...\n",
            "\n",
            "Section Found: {'Header 2': 'b. Semantic Understanding and Information Extraction'}\n",
            "Content Preview: ## b. Semantic Understanding and Information Extraction  \n",
            "- Use transformer-based NLP models (e.g., ...\n",
            "\n",
            "Section Found: {'Header 2': 'c. Summarization and Insight Generation'}\n",
            "Content Preview: ## c. Summarization and Insight Generation  \n",
            "- Generate multiple types of summaries:\n",
            "- o Short execu...\n",
            "\n",
            "Section Found: {'Header 2': 'd. Searchable Repository and Web Platform'}\n",
            "Content Preview: ## d. Searchable Repository and Web Platform  \n",
            "- Store processed documents and extracted metadata in...\n",
            "\n",
            "Section Found: {'Header 2': '4. Outcomes'}\n",
            "Content Preview: ## 4. Outcomes  \n",
            "The successful implementation of this project will deliver:  \n",
            "- An end-to-end  inte...\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from langchain_text_splitters import MarkdownHeaderTextSplitter\n",
        "\n",
        "# 1. Define which headers to split on (based on your PDF structure)\n",
        "headers_to_split_on = [\n",
        "    (\"#\", \"Header 1\"),      # Project Title\n",
        "    (\"##\", \"Header 2\"),     # Main Sections (Introduction, Methodology)\n",
        "    (\"###\", \"Header 3\"),    # Sub-sections (a. Ingestion, b. Semantic)\n",
        "]\n",
        "\n",
        "# 2. Initialize the splitter\n",
        "markdown_splitter = MarkdownHeaderTextSplitter(\n",
        "    headers_to_split_on=headers_to_split_on,\n",
        "    strip_headers=False # Keep the headers inside the text so the LLM sees them\n",
        ")\n",
        "\n",
        "# 3. Split the markdown content into logical sections\n",
        "section_docs = markdown_splitter.split_text(markdown_output)\n",
        "\n",
        "# Preview: Check the metadata to see if it captured your sections\n",
        "for doc in section_docs:\n",
        "    print(f\"Section Found: {doc.metadata}\")\n",
        "    print(f\"Content Preview: {doc.page_content[:100]}...\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rkwch6XM5G4k",
        "outputId": "f58ff66d-803b-4acb-d2d7-b6569f23d56f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "10"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(section_docs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b41gczoG3znN",
        "outputId": "71d1e5f8-078c-45de-984e-0a1e6d59b650"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "We now have 10 chunks, each tagged with its original section header!\n"
          ]
        }
      ],
      "source": [
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "\n",
        "# Use this to break down large sections while keeping them linked to their header\n",
        "child_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=1500,\n",
        "    chunk_overlap=150\n",
        ")\n",
        "\n",
        "final_chunks = child_splitter.split_documents(section_docs)\n",
        "\n",
        "print(f\"We now have {len(final_chunks)} chunks, each tagged with its original section header!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c27B5mNo5mF7"
      },
      "outputs": [],
      "source": [
        "# final_chunks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VDbAk6Q2XD-Z"
      },
      "source": [
        "# Stage 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eOBT0pPB6smu",
        "outputId": "f62bd659-6ed3-4f71-e70a-fdfb8552bb2b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: google-genai in /usr/local/lib/python3.12/dist-packages (1.59.0)\n",
            "Requirement already satisfied: pydantic in /usr/local/lib/python3.12/dist-packages (2.12.3)\n",
            "Collecting instructor\n",
            "  Downloading instructor-1.14.4-py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: anyio<5.0.0,>=4.8.0 in /usr/local/lib/python3.12/dist-packages (from google-genai) (4.12.1)\n",
            "Requirement already satisfied: google-auth<3.0.0,>=2.47.0 in /usr/local/lib/python3.12/dist-packages (from google-auth[requests]<3.0.0,>=2.47.0->google-genai) (2.47.0)\n",
            "Requirement already satisfied: httpx<1.0.0,>=0.28.1 in /usr/local/lib/python3.12/dist-packages (from google-genai) (0.28.1)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.28.1 in /usr/local/lib/python3.12/dist-packages (from google-genai) (2.32.5)\n",
            "Requirement already satisfied: tenacity<9.2.0,>=8.2.3 in /usr/local/lib/python3.12/dist-packages (from google-genai) (9.1.2)\n",
            "Requirement already satisfied: websockets<15.1.0,>=13.0.0 in /usr/local/lib/python3.12/dist-packages (from google-genai) (15.0.1)\n",
            "Requirement already satisfied: typing-extensions<5.0.0,>=4.11.0 in /usr/local/lib/python3.12/dist-packages (from google-genai) (4.15.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from google-genai) (1.9.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from google-genai) (1.3.1)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic) (2.41.4)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic) (0.4.2)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.9.1 in /usr/local/lib/python3.12/dist-packages (from instructor) (3.13.3)\n",
            "Collecting diskcache>=5.6.3 (from instructor)\n",
            "  Downloading diskcache-5.6.3-py3-none-any.whl.metadata (20 kB)\n",
            "Requirement already satisfied: docstring-parser<1.0,>=0.16 in /usr/local/lib/python3.12/dist-packages (from instructor) (0.17.0)\n",
            "Requirement already satisfied: jinja2<4.0.0,>=3.1.4 in /usr/local/lib/python3.12/dist-packages (from instructor) (3.1.6)\n",
            "Collecting jiter<0.12,>=0.6.1 (from instructor)\n",
            "  Downloading jiter-0.11.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.2 kB)\n",
            "Requirement already satisfied: openai<3.0.0,>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from instructor) (2.14.0)\n",
            "Collecting pre-commit>=4.3.0 (from instructor)\n",
            "  Downloading pre_commit-4.5.1-py2.py3-none-any.whl.metadata (1.2 kB)\n",
            "Requirement already satisfied: rich<15.0.0,>=13.7.0 in /usr/local/lib/python3.12/dist-packages (from instructor) (13.9.4)\n",
            "Collecting ty>=0.0.1a23 (from instructor)\n",
            "  Downloading ty-0.0.12-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.3 kB)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.9.0 in /usr/local/lib/python3.12/dist-packages (from instructor) (0.19.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.9.1->instructor) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.9.1->instructor) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.9.1->instructor) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.9.1->instructor) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.9.1->instructor) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.9.1->instructor) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.9.1->instructor) (1.22.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio<5.0.0,>=4.8.0->google-genai) (3.11)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from google-auth<3.0.0,>=2.47.0->google-auth[requests]<3.0.0,>=2.47.0->google-genai) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.12/dist-packages (from google-auth<3.0.0,>=2.47.0->google-auth[requests]<3.0.0,>=2.47.0->google-genai) (4.9.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0,>=0.28.1->google-genai) (2026.1.4)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0,>=0.28.1->google-genai) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1.0.0,>=0.28.1->google-genai) (0.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2<4.0.0,>=3.1.4->instructor) (3.0.3)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.12/dist-packages (from openai<3.0.0,>=2.0.0->instructor) (4.67.1)\n",
            "Collecting cfgv>=2.0.0 (from pre-commit>=4.3.0->instructor)\n",
            "  Downloading cfgv-3.5.0-py2.py3-none-any.whl.metadata (8.9 kB)\n",
            "Collecting identify>=1.0.0 (from pre-commit>=4.3.0->instructor)\n",
            "  Downloading identify-2.6.16-py2.py3-none-any.whl.metadata (4.4 kB)\n",
            "Collecting nodeenv>=0.11.1 (from pre-commit>=4.3.0->instructor)\n",
            "  Downloading nodeenv-1.10.0-py2.py3-none-any.whl.metadata (24 kB)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from pre-commit>=4.3.0->instructor) (6.0.3)\n",
            "Collecting virtualenv>=20.10.0 (from pre-commit>=4.3.0->instructor)\n",
            "  Downloading virtualenv-20.36.1-py3-none-any.whl.metadata (4.7 kB)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.28.1->google-genai) (3.4.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.28.1->google-genai) (2.5.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich<15.0.0,>=13.7.0->instructor) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich<15.0.0,>=13.7.0->instructor) (2.19.2)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.12/dist-packages (from typer<1.0.0,>=0.9.0->instructor) (8.3.1)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from typer<1.0.0,>=0.9.0->instructor) (1.5.4)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich<15.0.0,>=13.7.0->instructor) (0.1.2)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.12/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3.0.0,>=2.47.0->google-auth[requests]<3.0.0,>=2.47.0->google-genai) (0.6.1)\n",
            "Collecting distlib<1,>=0.3.7 (from virtualenv>=20.10.0->pre-commit>=4.3.0->instructor)\n",
            "  Downloading distlib-0.4.0-py2.py3-none-any.whl.metadata (5.2 kB)\n",
            "Requirement already satisfied: filelock<4,>=3.20.1 in /usr/local/lib/python3.12/dist-packages (from virtualenv>=20.10.0->pre-commit>=4.3.0->instructor) (3.20.2)\n",
            "Requirement already satisfied: platformdirs<5,>=3.9.1 in /usr/local/lib/python3.12/dist-packages (from virtualenv>=20.10.0->pre-commit>=4.3.0->instructor) (4.5.1)\n",
            "Downloading instructor-1.14.4-py3-none-any.whl (176 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m176.9/176.9 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading diskcache-5.6.3-py3-none-any.whl (45 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.5/45.5 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jiter-0.11.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (358 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m358.8/358.8 kB\u001b[0m \u001b[31m15.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pre_commit-4.5.1-py2.py3-none-any.whl (226 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m226.4/226.4 kB\u001b[0m \u001b[31m19.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ty-0.0.12-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (10.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.1/10.1 MB\u001b[0m \u001b[31m94.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading cfgv-3.5.0-py2.py3-none-any.whl (7.4 kB)\n",
            "Downloading identify-2.6.16-py2.py3-none-any.whl (99 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.2/99.2 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nodeenv-1.10.0-py2.py3-none-any.whl (23 kB)\n",
            "Downloading virtualenv-20.36.1-py3-none-any.whl (6.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.0/6.0 MB\u001b[0m \u001b[31m114.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading distlib-0.4.0-py2.py3-none-any.whl (469 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m469.0/469.0 kB\u001b[0m \u001b[31m32.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: distlib, virtualenv, ty, nodeenv, jiter, identify, diskcache, cfgv, pre-commit, instructor\n",
            "  Attempting uninstall: jiter\n",
            "    Found existing installation: jiter 0.12.0\n",
            "    Uninstalling jiter-0.12.0:\n",
            "      Successfully uninstalled jiter-0.12.0\n",
            "Successfully installed cfgv-3.5.0 diskcache-5.6.3 distlib-0.4.0 identify-2.6.16 instructor-1.14.4 jiter-0.11.1 nodeenv-1.10.0 pre-commit-4.5.1 ty-0.0.12 virtualenv-20.36.1\n"
          ]
        }
      ],
      "source": [
        "!pip install google-genai pydantic instructor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "qvK90ADvYG6b"
      },
      "outputs": [],
      "source": [
        "chunk_texts = [chunk.page_content for chunk in final_chunks]\n",
        "\n",
        "full_text_to_analyze = \"\\n--- NEW CHUNK ---\\n\".join(chunk_texts)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "_67eqUnCXpyN"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "os.environ['GOOGLE_API_KEY'] = userdata.get('GOOGLE_API_KEY')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "jdCii1qNkLbS"
      },
      "outputs": [],
      "source": [
        "# import google.generativeai as generativeai\n",
        "\n",
        "# for model in generativeai.list_models():\n",
        "#   print(model.name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "dKIPMh9RDyn0"
      },
      "outputs": [],
      "source": [
        "from google import genai\n",
        "from pydantic import BaseModel, Field\n",
        "from typing import List\n",
        "\n",
        "class Entity(BaseModel):\n",
        "    chunk_index: int = Field(description=\"The index of the chunk where this was found (0-9)\")\n",
        "    name: str = Field(description=\"The extracted entity (e.g., 'Itsolera', '2026-01-01')\")\n",
        "    type: str = Field(description=\"Organization, Date, Monetary Value, Legal Reference, or Stakeholder\")\n",
        "\n",
        "class Relationship(BaseModel):\n",
        "    chunk_index: int = Field(description=\"The index of the chunk where this connection was found\")\n",
        "    subject: str = Field(description=\"The entity performing the action/obligation\")\n",
        "    relation: str = Field(description=\"The connection (e.g., 'must deliver', 'partnered with', 'deadline for')\")\n",
        "    object: str = Field(description=\"The entity or task being acted upon\")\n",
        "\n",
        "class FullDocumentExtraction(BaseModel):\n",
        "    # Overall Document Semantics (Section 3.b Requirement)\n",
        "    document_intent: str = Field(description=\"High-level semantics: What is the purpose of this document?\")\n",
        "    topics: List[str] = Field(description=\"Topic Modeling: List major themes and subject areas found\")\n",
        "\n",
        "    # Granular Extraction (Section 3.b Requirement)\n",
        "    entities: List[Entity] = Field(description=\"Named Entity Recognition: Extracted key data points\")\n",
        "    relationships: List[Relationship] = Field(description=\"Detected obligations, dependencies, or responsibilities\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "cxz8QCJhFApa"
      },
      "outputs": [],
      "source": [
        "client = genai.Client()\n",
        "\n",
        "\n",
        "def extract_all_intelligence(full_text_to_analyze):\n",
        "  prompt = f\"\"\"\n",
        "    Analyze the following document which is split into numbered chunks.\n",
        "\n",
        "    TASK:\n",
        "    1. Identify the OVERALL INTENT of the Document (High-level Semantics).\n",
        "    2. List MAJOR THEMES of the Document (Topic Modeling).\n",
        "    2. Extract all ORGANIZATIONS, DATES, MONETARY VALUES, LEGAL REFERENCES and KEY STAKEHOLDERS (NER).\n",
        "    3. Detect all OBLIGATIONS, DEPENDENCIES and RELATIONSHIP between these entities (Relationship Detection).\n",
        "\n",
        "    CRITICAL: For every Entity and Relationship, note the CHUNK INDEX where it appeared.\n",
        "\n",
        "    DOCUMENT TEXT:\n",
        "    {full_text_to_analyze}\n",
        "    \"\"\"\n",
        "\n",
        "  response = client.models.generate_content(\n",
        "      model='gemini-3-flash-preview',\n",
        "      contents=prompt,\n",
        "      config={\n",
        "          'response_mime_type': 'application/json',\n",
        "          'response_schema': FullDocumentExtraction, # Uses your Pydantic model\n",
        "      },\n",
        "  )\n",
        "  return response.parsed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "N8tvQbS_HG7H"
      },
      "outputs": [],
      "source": [
        "all_intelligence = extract_all_intelligence(full_text_to_analyze)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xnMFg741Hd_2",
        "outputId": "74fb4e16-ac55-4f46-b7c8-0ec1427ae201"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "FullDocumentExtraction(document_intent='The document outlines a proposal and architectural overview for an Intelligent Document Understanding & Summarization Platform, designed to transform unstructured long-form text into structured, actionable insights for enterprise users.', topics=['Natural Language Processing', 'Information Extraction', 'Semantic Search', 'Automated Summarization', 'Enterprise Knowledge Management', 'Document Layout Analysis'], entities=[Entity(chunk_index=0, name='Intelligent Document Understanding & Summarization Platform', type='Stakeholder'), Entity(chunk_index=1, name='NLP', type='Legal Reference'), Entity(chunk_index=1, name='Researchers', type='Stakeholder'), Entity(chunk_index=1, name='Legal teams', type='Stakeholder'), Entity(chunk_index=1, name='Policy analysts', type='Stakeholder'), Entity(chunk_index=1, name='Corporate decision-makers', type='Stakeholder'), Entity(chunk_index=4, name='PDF', type='Legal Reference'), Entity(chunk_index=4, name='DOC', type='Legal Reference'), Entity(chunk_index=5, name='Longformer', type='Legal Reference'), Entity(chunk_index=5, name='BigBird', type='Legal Reference'), Entity(chunk_index=6, name='JSON', type='Legal Reference'), Entity(chunk_index=7, name='Flask', type='Legal Reference'), Entity(chunk_index=7, name='FastAPI', type='Legal Reference')], relationships=[Relationship(chunk_index=1, subject='The platform', relation='aims to address', object='challenge of extracting insights from large textual data'), Relationship(chunk_index=1, subject='The system', relation='supports', object='researchers and corporate decision-makers'), Relationship(chunk_index=3, subject='The proposed platform', relation='will be developed using', object='modular, scalable NLP architecture'), Relationship(chunk_index=4, subject='Document parsers', relation='extract', object='raw text while preserving layout cues'), Relationship(chunk_index=5, subject='Named Entity Recognition (NER)', relation='extracts', object='organizations, dates, and monetary values'), Relationship(chunk_index=6, subject='The platform', relation='outputs', object='results in structured JSON formats'), Relationship(chunk_index=7, subject='Flask or FastAPI', relation='is used to develop', object='a web-based interface for processing and visualization'), Relationship(chunk_index=8, subject='Project implementation', relation='delivers', object='a searchable document repository powered by NLP')])"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "all_intelligence"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GO-PYN_EHIIH"
      },
      "source": [
        "# Stage 3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZxvzQgxYY7yp"
      },
      "source": [
        "## Extract Actionable Insights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "VrYcW7GjXgz8"
      },
      "outputs": [],
      "source": [
        "from google import genai\n",
        "from pydantic import BaseModel, Field\n",
        "from typing import List\n",
        "\n",
        "class ActionableInsight(BaseModel):\n",
        "    chunk_index: int = Field(description=\"Chunk Number\")\n",
        "    type: str = Field(description=\"Risk, Deadline, Decision, or Recommendation\")\n",
        "    description: str = Field(description=\"Summary of the insight\")\n",
        "    entities: List[str] = Field(description=\"Stakeholders or organizations involved\")\n",
        "    date_or_value: str = Field(description=\"Deadlines or monetary values\")\n",
        "\n",
        "class ActionableInsightList(BaseModel):\n",
        "    insights: List[ActionableInsight]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "u359bI2lXzKX"
      },
      "outputs": [],
      "source": [
        "def generate_actionable_insights(full_text_to_analyze):\n",
        "  prompt = f\"\"\"\n",
        "    Analyze these document segments. For EVERY numbered chunk index, you MUST return at least one entry in the 'insights' list.\n",
        "\n",
        "    - If a chunk contains a Risk, Decision, Deadline or other Action: Extract it normally.\n",
        "    - If a chunk contains NO actionable insights: Set 'type' to \"N/A\", 'description' to \"N/A\", 'date_or_value' to \"N/A\" and 'entities' to [].\n",
        "    - Every chunk index from 0 to {len(chunk_texts)-1} must be represented in your output.\n",
        "\n",
        "    SEGMENTS:\n",
        "    {full_text_to_analyze}\n",
        "    \"\"\"\n",
        "  response = client.models.generate_content(\n",
        "      model='gemini-3-flash-preview',\n",
        "      contents=prompt,\n",
        "      config={\n",
        "          'response_mime_type': 'application/json',\n",
        "          'response_schema': ActionableInsightList, # Uses your Pydantic model\n",
        "      },\n",
        "  )\n",
        "  return response.parsed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "ioYMK0ZYJNSp"
      },
      "outputs": [],
      "source": [
        "all_insights = generate_actionable_insights(full_text_to_analyze)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x7wc8fikZ15s",
        "outputId": "dbe227ea-c238-4b3b-ada3-439cb52041f6"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "ActionableInsightList(insights=[ActionableInsight(chunk_index=0, type='N/A', description='N/A', entities=[], date_or_value='N/A'), ActionableInsight(chunk_index=1, type='Decision', description='Establishment of an end-to-end NLP solution for understanding and summarizing long-form unstructured documents.', entities=['NLP', 'legal teams', 'policy analysts', 'corporate decision-makers'], date_or_value='N/A'), ActionableInsight(chunk_index=2, type='Risk', description='Large documents often exceed the context window of standard language models, leading to potential loss of semantic meaning.', entities=['standard language models'], date_or_value='N/A'), ActionableInsight(chunk_index=3, type='Decision', description='Development of the platform using a modular and scalable NLP architecture.', entities=['NLP architecture'], date_or_value='N/A'), ActionableInsight(chunk_index=4, type='Decision', description='Support for PDF and DOC file ingestion with automatic logical segmentation and overlap chunking strategies.', entities=['PDF', 'DOC'], date_or_value='N/A'), ActionableInsight(chunk_index=5, type='Decision', description='Utilization of transformer-based models like Longformer or BigBird to capture document-level semantics and apply NER.', entities=['Longformer', 'BigBird', 'LLM'], date_or_value='N/A'), ActionableInsight(chunk_index=6, type='Decision', description='Extraction of actionable insights including key decisions, deadlines, and risks into structured JSON formats.', entities=['JSON'], date_or_value='N/A'), ActionableInsight(chunk_index=7, type='Decision', description='Implementation of a web-based interface using Flask or FastAPI with access control and audit logging.', entities=['Flask', 'FastAPI'], date_or_value='N/A'), ActionableInsight(chunk_index=8, type='Recommendation', description='Delivery of a searchable, queryable document repository suitable for enterprise and legal use cases.', entities=['enterprise', 'research', 'legal'], date_or_value='N/A'), ActionableInsight(chunk_index=9, type='Decision', description='The platform will serve as a foundation for enterprise document automation and AI-driven knowledge management.', entities=['enterprise document automation', 'AI-driven knowledge management'], date_or_value='N/A')])"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "all_insights"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r-VLMwrWndCk"
      },
      "source": [
        "## Extract Summaries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "G6HZ__5sneeq"
      },
      "outputs": [],
      "source": [
        "class SectionSummary(BaseModel):\n",
        "    section_header: str = Field(description=\"The heading or title of the document section\")\n",
        "    summary_text: str = Field(description=\"A 2-3 sentence summary of this specific section\")\n",
        "\n",
        "# 2. Update the main report model\n",
        "class DocumentSummaries(BaseModel):\n",
        "    executive_summary: str = Field(description=\"A high-level overview for decision-makers\")\n",
        "    technical_summary: str = Field(description=\"Detailed summary focusing on technical dependencies\")\n",
        "    # Using the specific model here fixes the ValueError\n",
        "    section_summaries: List[SectionSummary]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "93EIk7QtnfI4"
      },
      "outputs": [],
      "source": [
        "def generate_final_summaries(insights_list, original_text):\n",
        "    # We pass the list of extracted insights as a helper to the model\n",
        "    # This ensures the summary doesn't miss the specific risks/deadlines we found\n",
        "    prompt = f\"\"\"\n",
        "    Using the following extracted insights and the original text, generate three types of summaries.\n",
        "\n",
        "    EXTRACTED INSIGHTS:\n",
        "    {insights_list}\n",
        "\n",
        "    ORIGINAL TEXT:\n",
        "    {original_text[:10000]} # Using a large window\n",
        "\n",
        "    REQUIREMENTS:\n",
        "    1. Executive: 3-5 sentences, high-level.\n",
        "    2. Technical: Detailed, focusing on dependencies.\n",
        "    3. Section-wise: A summary for each major header identified.\n",
        "    \"\"\"\n",
        "\n",
        "    # Using your working extraction pattern\n",
        "    response = client.models.generate_content(\n",
        "        model=\"gemini-3-flash-preview\",\n",
        "        contents=prompt,\n",
        "        config={\n",
        "            \"response_mime_type\": \"application/json\",\n",
        "            \"response_schema\": DocumentSummaries,\n",
        "        }\n",
        "    )\n",
        "    return response.parsed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "sNZ-5tQ_n-rO"
      },
      "outputs": [],
      "source": [
        "final_report = generate_final_summaries(all_insights, full_text_to_analyze)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B6SYGsWcpJSM",
        "outputId": "17bc2893-a244-48dd-aff1-fe5cfb86a424"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "DocumentSummaries(executive_summary='The Intelligent Document Understanding and Summarization Platform is an end-to-end NLP solution designed to convert massive volumes of unstructured text into actionable insights for legal, research, and corporate professionals. By leveraging advanced semantic analysis, the system identifies key decisions, risks, and deadlines within long-form documents that often exceed standard processing limits. This platform provides a centralized, queryable repository that reduces cognitive load and enhances decision-making across the enterprise. It serves as a foundation for document automation, transforming static files into a structured, searchable knowledge base.', technical_summary='The technical architecture is built on a modular NLP framework designed to process long-context documents using transformer models like Longformer, BigBird, and modern LLM pipelines. To prevent loss of semantic meaning, the system implements automatic logical segmentation and overlap chunking strategies for PDF and DOC files. Key technical components include Named Entity Recognition (NER) for extracting stakeholders and monetary values, relationship detection for dependencies, and a structured JSON output layer for actionable insights. The application is served via Flask or FastAPI, providing a web-based interface with document versioning, access control, and a searchable metadata repository.', section_summaries=[SectionSummary(section_header='Introduction', summary_text='This section introduces the platform as a solution for extracting meaningful insights from massive volumes of complex, unstructured textual data. It identifies the target audience as legal teams, policy analysts, and corporate decision-makers who need to transform documents into actionable knowledge.'), SectionSummary(section_header='Problem Statement', summary_text='The document highlights major limitations in current NLP tools, specifically the context window constraints of standard models and the loss of critical details in generic summaries. It emphasizes the need for a system that can explicitly extract deadlines, risks, and obligations while providing queryable access to historical content.'), SectionSummary(section_header='Methodology', summary_text='A modular and scalable NLP architecture is proposed to maintain semantic consistency across long documents. The methodology focuses on producing structured outputs that can seamlessly integrate with enterprise workflows and analytics systems.'), SectionSummary(section_header='Document Ingestion and Segmentation', summary_text='This technical phase involves parsing PDF and DOC files into raw text while preserving layout cues. Overlap chunking and semantic segmentation models are utilized to manage long documents without losing contextual continuity between sections.'), SectionSummary(section_header='Semantic Understanding and Information Extraction', summary_text='The platform uses specialized transformer models like Longformer and BigBird to capture document-level themes and entities. NER and relationship detection are applied to map stakeholders, legal references, and dependencies across the text.'), SectionSummary(section_header='Summarization and Insight Generation', summary_text='The system generates executive, technical, and section-wise summaries to cater to different user needs. Actionable insights such as key decisions and risks are extracted into structured JSON formats for easier data processing.'), SectionSummary(section_header='Searchable Repository and Web Platform', summary_text='A web interface built with Flask or FastAPI provides users with tools for document uploading, visualization, and natural language querying. The platform includes enterprise-ready features like access control, versioning, and audit logging.'), SectionSummary(section_header='Outcomes', summary_text='The final implementation delivers a comprehensive document intelligence platform capable of multi-level summarization and structured insight extraction. It establishes a foundation for AI-driven knowledge management and enterprise-wide decision support.')])"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "final_report"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yNIafhRskfzI"
      },
      "outputs": [],
      "source": [
        "# chunk_texts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ShEbgEammYqP"
      },
      "outputs": [],
      "source": [
        "# final_report.executive_summary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q-7ai_EEma5P"
      },
      "outputs": [],
      "source": [
        "# final_report.technical_summary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fz-FognwmmwW"
      },
      "outputs": [],
      "source": [
        "# final_report.section_summaries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "CiUllRJ1yI6o"
      },
      "outputs": [],
      "source": [
        "def generate_embedding(texts: list[str]):\n",
        "  result = client.models.embed_content(\n",
        "      model=\"models/text-embedding-004\",\n",
        "      contents=texts\n",
        "  )\n",
        "  return [e.values for e in result.embeddings]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zb2l6YG7y-Ys"
      },
      "outputs": [],
      "source": [
        "# embeddings = generate_embedding(chunk_texts)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RgV5D5u8zEO7"
      },
      "outputs": [],
      "source": [
        "# embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yApzcAKtmvHA"
      },
      "source": [
        "# Storing in MongoDB"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t4fJeGnq0wMd",
        "outputId": "c3ac7683-98e9-4a90-9e85-efc8f88ca673"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pymongo in /usr/local/lib/python3.12/dist-packages (4.16.0)\n",
            "Requirement already satisfied: dnspython<3.0.0,>=2.6.1 in /usr/local/lib/python3.12/dist-packages (from pymongo) (2.8.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install pymongo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QuUawiQv0xtP",
        "outputId": "db003740-61f9-4d10-e946-3d807fb50329"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Pinged your deployment. You successfully connected to MongoDB!\n"
          ]
        }
      ],
      "source": [
        "from pymongo.mongo_client import MongoClient\n",
        "from pymongo.server_api import ServerApi\n",
        "\n",
        "uri = MONGODB_URI\n",
        "\n",
        "# Create a new client and connect to the server\n",
        "mongo_client = MongoClient(uri, server_api=ServerApi('1'))\n",
        "\n",
        "# Send a ping to confirm a successful connection\n",
        "try:\n",
        "    mongo_client.admin.command('ping')\n",
        "    print(\"Pinged your deployment. You successfully connected to MongoDB!\")\n",
        "except Exception as e:\n",
        "    print(e)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "m8wL7hxHVzQ3"
      },
      "outputs": [],
      "source": [
        "db = mongo_client[\"TeamAlpha_DB\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "zqU9BRNVmvhG"
      },
      "outputs": [],
      "source": [
        "import uuid\n",
        "from datetime import datetime\n",
        "\n",
        "# # Initialize Client\n",
        "\n",
        "def final_storage_logic(doc_summaries, insight_list, intelligence, raw_chunks, filename, owner):\n",
        "    \"\"\"\n",
        "    Combines high-level summaries with granular raw chunks and actionable insights.\n",
        "    \"\"\"\n",
        "    doc_id = str(uuid.uuid4())\n",
        "\n",
        "\n",
        "    insights_by_chunk = {}\n",
        "    for ins in insight_list.insights:\n",
        "        idx = ins.chunk_index\n",
        "        if idx not in insights_by_chunk:\n",
        "            insights_by_chunk[idx] = []\n",
        "\n",
        "        # Only add to list if it's not a placeholder \"N/A\"\n",
        "        if ins.type.upper() != \"N/A\":\n",
        "            insights_by_chunk[idx].append(ins.model_dump())\n",
        "\n",
        "\n",
        "    # 2. Pre-process Semantic Extraction by Chunk Index (Section 3.b)\n",
        "    entities_by_chunk = {}\n",
        "    for ent in intelligence.entities:\n",
        "        idx = ent.chunk_index\n",
        "        if idx not in entities_by_chunk:\n",
        "            entities_by_chunk[idx] = []\n",
        "        entities_by_chunk[idx].append(ent.model_dump())\n",
        "\n",
        "    rels_by_chunk = {}\n",
        "    for rel in intelligence.relationships:\n",
        "        idx = rel.chunk_index\n",
        "        if idx not in rels_by_chunk:\n",
        "            rels_by_chunk[idx] = []\n",
        "        rels_by_chunk[idx].append(rel.model_dump())\n",
        "\n",
        "\n",
        "    # 1. Prepare Parent Document (Global Metadata & Summaries)\n",
        "    # This fulfills your requirement for \"Summary Viewing\" and \"Audit Logging\"\n",
        "    parent_doc = {\n",
        "        \"_id\": doc_id,\n",
        "        \"filename\": filename,\n",
        "        \"version\": 1,\n",
        "        \"upload_date\": datetime.utcnow().isoformat(),\n",
        "        \"owner\": owner,\n",
        "        \"document_intent\": intelligence.document_intent,\n",
        "        \"major_themes\": intelligence.topics,\n",
        "        \"executive_summary\": doc_summaries.executive_summary,\n",
        "        \"technical_summary\": doc_summaries.technical_summary,\n",
        "        \"audit_log\": [\n",
        "            {\"action\": \"initial_processing\", \"user\": owner, \"time\": datetime.utcnow().isoformat()}\n",
        "        ]\n",
        "    }\n",
        "\n",
        "    # 2. Prepare Child Chunks (Searchable Units)\n",
        "    # We map raw 1500-char chunks to their nearest AI-generated Section Summary\n",
        "    child_chunks = []\n",
        "\n",
        "    embeddings = generate_embedding(raw_chunks)\n",
        "\n",
        "    for i, chunk_text in enumerate(raw_chunks):\n",
        "        # Heuristic: Find which section header belongs to this chunk\n",
        "        matched_header = \"General\"\n",
        "        matched_summary = \"N/A\"\n",
        "\n",
        "        for section in doc_summaries.section_summaries:\n",
        "            if section.section_header.lower() in chunk_text.lower():\n",
        "                matched_header = section.section_header\n",
        "                matched_summary = section.summary_text\n",
        "                break\n",
        "\n",
        "        current_insights = insights_by_chunk.get(i, [])\n",
        "        flat_types = list(set([ins['type'] for ins in current_insights]))\n",
        "\n",
        "        chunk_entry = {\n",
        "            \"_id\": str(uuid.uuid4()),\n",
        "            \"parent_doc_id\": doc_id,\n",
        "            \"chunk_index\": i,\n",
        "            \"section_header\": matched_header,\n",
        "            \"chunk_text\": chunk_text,  # The actual raw text for RAG retrieval\n",
        "            \"embedding\": embeddings[i],  # TODO: Call your embedding function here\n",
        "            \"entities\": entities_by_chunk.get(i, []),\n",
        "            \"relationships\": rels_by_chunk.get(i, []),\n",
        "            \"section_summary\": matched_summary,\n",
        "            \"actionable_insights\": current_insights,\n",
        "            \"insight_types\": flat_types,\n",
        "        }\n",
        "        child_chunks.append(chunk_entry)\n",
        "\n",
        "    # 3. Atomic Inserts into MongoDB Atlas\n",
        "    db.documents.insert_one(parent_doc)\n",
        "    db.chunks.insert_many(child_chunks)\n",
        "\n",
        "\n",
        "    print(f\"Document '{filename}' stored. Parent ID: {doc_id} | Chunks: {len(child_chunks)}\")\n",
        "    return doc_id\n",
        "\n",
        "# --- Execution ---\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 142
        },
        "id": "T9_TivRJn8V3",
        "outputId": "96e3acc2-90a7-4d6a-afcd-49f6286cdb63"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipython-input-4256641421.py:46: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  \"upload_date\": datetime.utcnow().isoformat(),\n",
            "/tmp/ipython-input-4256641421.py:53: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  {\"action\": \"initial_processing\", \"user\": owner, \"time\": datetime.utcnow().isoformat()}\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Document 'TeamAlpha.pdf' stored. Parent ID: aa7170ef-55f8-4ef3-9d73-15c54f774858 | Chunks: 10\n"
          ]
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'aa7170ef-55f8-4ef3-9d73-15c54f774858'"
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "final_storage_logic(doc_summaries=final_report, insight_list=all_insights, intelligence=all_intelligence, raw_chunks=chunk_texts, filename=\"TeamAlpha.pdf\", owner=\"TeamAlpha_User\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5BlTlUc9oy3e"
      },
      "outputs": [],
      "source": [
        "# for i, chunk_text in enumerate(chunk_texts):\n",
        "#   # Heuristic: Find which section header belongs to this chunk\n",
        "#   matched_header = \"General\"\n",
        "#   matched_summary = \"N/A\"\n",
        "\n",
        "#   for section in final_report.section_summaries:\n",
        "#       if section.section_header.lower() in chunk_text.lower():\n",
        "#           matched_header = section.section_header\n",
        "#           matched_summary = section.summary_text\n",
        "#           print(matched_header)\n",
        "#           print(matched_summary)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fuBHoJb6psWP",
        "outputId": "d4396b97-3b11-4f88-f58a-cbf172d562cd"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "8"
            ]
          },
          "execution_count": 46,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# len(final_report.section_summaries)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qtivEbOh9Xav"
      },
      "outputs": [],
      "source": [
        "# def ask_document_repository(query: str, num_results: int=3):\n",
        "#   query_vector = generate_embedding([query])[0]\n",
        "\n",
        "#   pipeline = [\n",
        "#         {\n",
        "#             \"$vectorSearch\": {\n",
        "#                 \"index\": \"vector_index\", # The name you gave your index in Atlas\n",
        "#                 \"path\": \"embedding\",\n",
        "#                 \"queryVector\": query_vector,\n",
        "#                 \"numCandidates\": 100,\n",
        "#                 \"limit\": num_results\n",
        "#             }\n",
        "#         },\n",
        "#         {\n",
        "#             # Project only necessary fields to save memory\n",
        "#             \"$project\": {\n",
        "#                 \"chunk_text\": 1,\n",
        "#                 \"section_header\": 1,\n",
        "#                 \"score\": {\"$meta\": \"vectorSearchScore\"}\n",
        "#             }\n",
        "#         }\n",
        "#     ]\n",
        "\n",
        "#   results = list(db.chunks.aggregate(pipeline))\n",
        "\n",
        "#   context_text = \"\\n\\n\".join([\n",
        "#       f\"SOURCE (Section: {r['section_header']}):\\n{r['chunk_text']}\"\n",
        "#       for r in results\n",
        "#   ])\n",
        "\n",
        "#   return context_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 157
        },
        "id": "z-IP-PId-Eaf",
        "outputId": "906f651a-58fe-4ae7-c1cc-eef920652dcd"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'SOURCE (Section: Summarization and Insight Generation):\\n## c. Summarization and Insight Generation  \\n- Generate multiple types of summaries:\\n- o Short executive summary for quick understanding\\n- o Detailed summary preserving technical depth\\n- o Section-wise summaries aligned with document structure\\n- Extract actionable insights , including:\\n- o Key decisions and conclusions\\n- o Deadlines and timelines\\n- o Identified risks and constraints\\n- o Recommendations and next steps\\n- Output results in structured formats (JSON) alongside human-readable summaries.  \\n<!-- image -->\\n\\nSOURCE (Section: Introduction):\\n## 1. Introduction  \\nOrganizations today generate and consume massive volumes of unstructured textual data in the form of reports, policies, research papers, legal contracts, compliance documents, and strategic briefs. While digital storage has made access easier, extracting meaningful insights from these large  and  complex  documents  remains  a  significant  challenge.  Professionals  often  spend considerable time reading, summarizing, and interpreting documents to identify key decisions, risks, deadlines, and recommendations.  \\nThe Intelligent  Document Understanding &amp; Summarization Platform aims  to  address  this challenge by providing an end-to-end Natural Language Processing (NLP) solution capable of understanding,  analyzing,  and  summarizing  long-form  documents.  The  platform  goes  beyond basic  text  summarization  by  preserving  semantic  context  across  sections,  extracting  structured insights, and enabling intelligent search through a document repository. Designed as a web-based enterprise solution, the system supports researchers, legal teams, policy analysts, and corporate decision-makers in transforming unstructured documents into actionable knowledge.\\n\\nSOURCE (Section: Semantic Understanding and Information Extraction):\\n## b. Semantic Understanding and Information Extraction  \\n- Use transformer-based NLP models (e.g., Longformer, BigBird, or LLM-based pipelines) to capture document-level semantics.\\n- Perform topic modeling to identify major themes and subject areas.\\n- Apply Named Entity Recognition (NER) to extract entities such as organizations, dates, monetary values, legal references, and key stakeholders.\\n- Detect  relationships  between  entities  to  understand  dependencies,  obligations,  and responsibilities.'"
            ]
          },
          "execution_count": 113,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# ask_document_repository(\"What to do in summarization and insight generation section?\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OadNX1htDHpA"
      },
      "source": [
        "# Building Self Query Retriever"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "t8ZNgzI8DKQr"
      },
      "outputs": [],
      "source": [
        "from langchain_google_genai import GoogleGenerativeAI, GoogleGenerativeAIEmbeddings\n",
        "from langchain_classic.retrievers.self_query.base import SelfQueryRetriever\n",
        "from langchain_classic.chains.query_constructor.base import AttributeInfo\n",
        "from langchain_mongodb import MongoDBAtlasVectorSearch\n",
        "\n",
        "metadata_field_info = [\n",
        "    AttributeInfo(\n",
        "        name=\"section_header\",\n",
        "        description=\"The specific header or title of the section (e.g., Introduction, Methodology)\",\n",
        "        type=\"string\",\n",
        "    ),\n",
        "    AttributeInfo(\n",
        "        name=\"insight_types\",\n",
        "        description=\"The category of insight: Risk, Deadline, Decision, or Recommendation\",\n",
        "        type=\"string\",\n",
        "    ),\n",
        "    AttributeInfo(\n",
        "        name=\"parent_doc_id\",\n",
        "        description=\"The unique ID of the document to filter by a specific file\",\n",
        "        type=\"string\",\n",
        "    ),\n",
        "\n",
        "    AttributeInfo(\n",
        "        name=\"entities.name\",\n",
        "        description=\"Names of specific entities mentioned (e.g., 'NLP', 'Researchers')\",\n",
        "        type=\"string\",\n",
        "    ),\n",
        "    AttributeInfo(\n",
        "        name=\"entities.type\",\n",
        "        description=\"The category of the entity (e.g., 'Stakeholder', 'Legal Reference')\",\n",
        "        type=\"string\",\n",
        "    ),\n",
        "    AttributeInfo(\n",
        "        name=\"relationships.relation\",\n",
        "        description=\"The type of connection detected (e.g., 'aims to address', 'supports')\",\n",
        "        type=\"string\",\n",
        "    )\n",
        "]\n",
        "\n",
        "document_content_description = \"A collection of long-form technical and legal document chunks with extracted insights, entities, and relationships.\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "Yd8jXw-TEvpI"
      },
      "outputs": [],
      "source": [
        "embedding_model = GoogleGenerativeAIEmbeddings(\n",
        "    model=\"models/text-embedding-004\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "E9ViAyszD34x"
      },
      "outputs": [],
      "source": [
        "llm = GoogleGenerativeAI(model=\"models/gemma-3-27b-it\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "gD_yI3LsEGmi"
      },
      "outputs": [],
      "source": [
        "vector_store = MongoDBAtlasVectorSearch(\n",
        "    collection=db.chunks,\n",
        "    embedding=embedding_model, # Your text-embedding-004 wrapper\n",
        "    index_name=\"vector_index\",\n",
        "    text_key=\"chunk_text\",\n",
        "    relevance_score_fn=\"cosine\",\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DifU7Z8eGPAN"
      },
      "outputs": [],
      "source": [
        "# vector_store.similarity_search(\"What to do in summarization and insight generation section?\", k=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "VMymuUm7E3o2"
      },
      "outputs": [],
      "source": [
        "self_query_retriever = SelfQueryRetriever.from_llm(\n",
        "    llm=llm,\n",
        "    vectorstore=vector_store,\n",
        "    document_contents=document_content_description,\n",
        "    metadata_field_info=metadata_field_info,\n",
        "    verbose=True # Helpful to see the \"Query Translation\" in the notebook\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yiPI4ry5FADX",
        "outputId": "6c83386f-bbf5-4f53-ebe0-631fcd296c39"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[General] ['NLP', 'Researchers', 'Legal teams', 'Policy analysts', 'Corporate decision-makers'] ## Intelligent Document Understanding &amp; Summarization Platform...\n",
            "[Introduction] ['Longformer', 'BigBird'] ## a. Document Ingestion and Segmentation  \n",
            "- Support uploading of PDF and DOC files (single or mult...\n"
          ]
        }
      ],
      "source": [
        "# Test: Combined Metadata + Semantic Search\n",
        "query = \"Show me chunks that mention either the stakeholder Researchers or the legal reference BigBird.\"\n",
        "docs = self_query_retriever.invoke(query)\n",
        "\n",
        "for doc in docs:\n",
        "    print(f\"[{doc.metadata['section_header']}] {[entity['name'] for entity in doc.metadata.get('entities', [])]} {doc.page_content[:100]}...\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "iORyOjnYWPI1"
      },
      "outputs": [],
      "source": [
        "def format_docs_with_metadata(docs):\n",
        "    formatted = []\n",
        "    for doc in docs:\n",
        "        # 1. Extract the rich metadata we stored in MongoDB\n",
        "        header = doc.metadata.get(\"section_header\", \"General\")\n",
        "        insights = doc.metadata.get(\"insight_types\", [])\n",
        "\n",
        "        # 2. Extract our new Semantic Intelligence (Section 3.b)\n",
        "        entities = doc.metadata.get(\"entities\", [])\n",
        "        relationships = doc.metadata.get(\"relationships\", [])\n",
        "\n",
        "        # 3. Build the context block for the LLM\n",
        "        text = f\"--- DOCUMENT SECTION: {header} ---\\n\"\n",
        "\n",
        "        if insights:\n",
        "            text += f\"CATEGORIES: {', '.join(insights)}\\n\"\n",
        "\n",
        "        if entities:\n",
        "            entity_list = [f\"{e['name']} ({e['type']})\" for e in entities]\n",
        "            text += f\"KEY ENTITIES: {', '.join(entity_list)}\\n\"\n",
        "\n",
        "        if relationships:\n",
        "            rel_list = [f\"{r['subject']} -> {r['relation']} -> {r['object']}\" for r in relationships]\n",
        "            text += f\"DETECTED OBLIGATIONS: {'; '.join(rel_list)}\\n\"\n",
        "\n",
        "        text += f\"RAW CONTENT: {doc.page_content}\\n\"\n",
        "\n",
        "        formatted.append(text)\n",
        "\n",
        "    return \"\\n\\n\".join(formatted)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "TsVorITSWb_M"
      },
      "outputs": [],
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "\n",
        "template = \"\"\"\n",
        "You are the Document Intelligence Engine.\n",
        "You are provided with specific document chunks and their associated metadata (Risks, Decisions, Entities, and Obligations etc.).\n",
        "\n",
        "TASK: {task_instruction}\n",
        "\n",
        "CONTEXT FROM DOCUMENT:\n",
        "{context}\n",
        "\n",
        "STRICT GUIDELINES:\n",
        "1. Use ONLY the provided context.\n",
        "2. Pay special attention to 'KEY ENTITIES' and 'DETECTED OBLIGATIONS' metadata to identify stakeholders and responsibilities accurately.\n",
        "3. If you are summarizing, use professional bullet points.\n",
        "4. If you are answering a search, provide a concise 2-3 sentence explanation followed by the evidence.\n",
        "5. Always reference the 'Section Header' for each point.\n",
        "\n",
        "FINAL OUTPUT:\n",
        "\"\"\"\n",
        "\n",
        "prompt = ChatPromptTemplate.from_template(template)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "CVX8TI0gWmt8"
      },
      "outputs": [],
      "source": [
        "def generate_intelligence(retrieved_docs, mode=\"search\", user_query=None):\n",
        "    # Format the context string (using the logic we discussed earlier)\n",
        "    context_text = format_docs_with_metadata(retrieved_docs)\n",
        "\n",
        "    # Switch instructions based on the mode\n",
        "    if mode == \"dashboard\":\n",
        "        instruction = \"\"\"\n",
        "        Synthesize a high-level executive dashboard from these chunks.\n",
        "        You must highlight:\n",
        "        1. KEY DECISIONS & RISKS: From the actionable insights.\n",
        "        2. STAKEHOLDERS & OBLIGATIONS: From the entities and detected relationships.\n",
        "        3. PRIMARY THEMES: Based on the content and headers.\n",
        "\n",
        "        Format the output for a quick professional briefing.\n",
        "        \"\"\"\n",
        "    else:\n",
        "        instruction = f\"Answer the following user search query: {user_query}\"\n",
        "\n",
        "    # Run the chain\n",
        "    chain = prompt | llm | StrOutputParser()\n",
        "    return chain.invoke({\"task_instruction\": instruction, \"context\": context_text})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LDqzMZSsWz5H",
        "outputId": "91dff328-da7e-4096-ef5f-be23d792586b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--- DASHBOARD VIEW ---\n",
            "## Executive Briefing: Document Intelligence Platform\n",
            "\n",
            "**Date:** October 26, 2023\n",
            "\n",
            "**Subject:** High-Level Overview of Document Intelligence Platform Capabilities\n",
            "\n",
            "---\n",
            "\n",
            "**1. Key Decisions & Risks**\n",
            "\n",
            "*   **Decision: Platform Development** – A decision has been made to develop an intelligent document understanding and summarization platform (General).\n",
            "*   **Risk: Long Document Handling** – The platform addresses the challenge of extracting insights from large textual data, implying a risk associated with processing and analyzing extensive documents (General).\n",
            "*   **Technology Choices:** Decisions involve leveraging models like Longformer and BigBird for document processing (Introduction).\n",
            "\n",
            "**2. Stakeholders & Obligations**\n",
            "\n",
            "*   **Key Stakeholders:** Researchers, Legal teams, Policy analysts, and Corporate decision-makers are central to the platform’s utility (General).\n",
            "*   **Platform Obligations:**\n",
            "    *   The platform aims to extract insights from large textual data (General).\n",
            "    *   The system supports researchers and corporate decision-makers (General).\n",
            "    *   Named Entity Recognition (NER) extracts key data points like organizations, dates, and monetary values (Introduction).\n",
            "\n",
            "**3. Primary Themes**\n",
            "\n",
            "*   **Document Processing:** The platform focuses on ingesting, segmenting, and chunking documents for effective analysis (Introduction).\n",
            "*   **Intelligent Understanding:** Core functionality centers around intelligent document understanding and summarization (General).\n",
            "*   **Technical Foundation:** The platform utilizes advanced models and techniques (Longformer, BigBird, NER) to achieve its objectives (Introduction).\n",
            "\n",
            "\n",
            "\n",
            "---\n"
          ]
        }
      ],
      "source": [
        "dashboard_docs = self_query_retriever.invoke(\"Show me chunks that mention either the stakeholder Researchers or the legal reference BigBird.\")\n",
        "\n",
        "# 2. Generate the summary for the UI\n",
        "dashboard_summary = generate_intelligence(dashboard_docs, mode=\"dashboard\")\n",
        "\n",
        "print(\"--- DASHBOARD VIEW ---\")\n",
        "print(dashboard_summary)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OKsiD4lFXu4u",
        "outputId": "41a207ca-e8cd-485e-c774-a5dd15c8b7f7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--- SEARCH RESULT ---\n",
            "The following stakeholders are involved with the Intelligent Document Understanding & Summarization Platform: researchers, legal teams, policy analysts, and corporate decision-makers. The platform itself is also a key entity, and aims to support researchers and corporate decision-makers in extracting insights from large textual data. \n",
            "\n",
            "Here's the evidence:\n",
            "\n",
            "*   **General** - KEY ENTITIES: Intelligent Document Understanding & Summarization Platform (Stakeholder)\n",
            "*   **General** - KEY ENTITIES: Researchers (Stakeholder), Legal teams (Stakeholder), Policy analysts (Stakeholder), Corporate decision-makers (Stakeholder)\n",
            "*   **General** - DETECTED OBLIGATIONS: The system -> supports -> researchers and corporate decision-makers\n"
          ]
        }
      ],
      "source": [
        "user_input = \"What stackholders are involved?\"\n",
        "\n",
        "# 1. Use Self-Query to find only relevant chunks\n",
        "search_docs = self_query_retriever.invoke(user_input)\n",
        "\n",
        "# 2. Generate a grounded answer\n",
        "search_answer = generate_intelligence(search_docs, mode=\"search\", user_query=user_input)\n",
        "\n",
        "print(\"--- SEARCH RESULT ---\")\n",
        "print(search_answer)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
